---
title: Finetuning LLM's at 2x speed with UnSloth
slug: finetuning-llm-unsloth
datetime: 2024-04-01T12:35:01.255Z
draft: false
type: post
tags:
  - LLM
  - GenAI
  - finetuning

ogImage: ""
---

# LLMs and Need of Finetuning
Alright, let’s talk big language models (LLMs) – they're everywhere, and it's like a new one pops up every day. After everyone started sharing these LLMs like hotcakes, places like Hugging Face are getting flooded with a thousand new models daily. But why the fuss about finetuning?

So here's the deal: models like Mistral, LLaMA, Open Hermes, or even GPT, they're smart but kind of generalists. They know a bit of everything, which is cool, but sometimes you need an expert, right? Imagine you're building a bot to crank out code or maybe a chatbot that knows health stuff inside and out. You want it to chat in a certain way, focusing on what matters for your project. That's where finetuning jumps in. It's like custom tailoring for your AI – making sure it gets your vibe and talks right for your audience.

# Finetuning: The What and How

Large language model (LLM) fine-tuning is the process of taking pre-trained models like Mistral/Open Hermes, even GPT  and further training them on smaller, specific datasets to refine their capabilities and improve performance in a particular task or domain. Fine-tuning is about turning general-purpose models and turning them into specialized models. It bridges the gap between generic pre-trained models and the unique requirements of specific applications, ensuring that the language model aligns closely with human expectations

Finetuning is not just one thing; it’s got flavors – LoRA, QLoRA, and the newbie, DORA. It's all about tweaking those model knobs to get the perfect response style, knowledge level, you name it.

But here’s the kicker: finetuning is a beast. It chews up a ton of memory and needs some serious computing muscle. Not exactly something you can run on your old laptop.But few tools and dataset size plays a key role, lets say i finetuned my own Zephyr Model with System Chat Dataset on Colab t4 in around 25 mins, yes thats all it took as its small dataset